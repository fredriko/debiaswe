{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Tutorial: Quantifying and Reducing Gender Stereotypes in Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring fairness in algorithmically-driven decision-making is important to avoid inadvertent cases of bias and perpetuation of harmful stereotypes. However, modern natural language processing techniques, which learn model parameters based on data, might rely on implicit biases presented in the data to make undesirable stereotypical associations. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. Recent results ([1](https://arxiv.org/abs/1607.06520), [2](https://arxiv.org/abs/1608.07187)) show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because of their widespread use, as we describe, often tends to amplify these biases. \n",
    "\n",
    "In the following, we provide step-by-step instructions to demonstrate and quanitfy the biases in word embedding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup:\n",
    "# Clone the code repository from https://github.com/tolga-b/debiaswe.git\n",
    "# mkdir debiaswe_tutorial\n",
    "# cd debiaswe_tutorial\n",
    "# git clone https://github.com/tolga-b/debiaswe.git\n",
    "\n",
    "# To reduce the time of downloading data, we provide as subset of GoogleNews-vectors in the following location:\n",
    "# https://drive.google.com/file/d/1NH6jcrg8SXbnhpIXRIXF_-KUE7wGxGaG/view?usp=sharing\n",
    "\n",
    "# For full embeddings:\n",
    "# Download embeddings at https://github.com/tolga-b/debiaswe and put them on the following directory\n",
    "# embeddings/GoogleNews-vectors-negative300-hard-debiased.bin\n",
    "# embeddings/GoogleNews-vectors-negative300.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import debiaswe as dwe\n",
    "import debiaswe.we as we\n",
    "from debiaswe.we import WordEmbedding\n",
    "from debiaswe.data import load_professions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Gender Bias in Word Embedding\n",
    "\n",
    "\n",
    "### Step 1: Load data\n",
    "We first load the word embedding trained on a corpus of Google News texts consisting of 3 million English words and terms. The embedding maps each word into a 300-dimension vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/sv-word2vec-vectors-nlpl-eu-69/model.txt\n",
      "(3010472, 100)\n",
      "3010472 words of dimension 100 : </s>, ., ,, i, ..., kuinyogashiten, 05-04-11, m2016-01-10, rollabudgetgratis\n",
      "3010472 words of dimension 100 : </s>, ., ,, i, ..., kuinyogashiten, 05-04-11, m2016-01-10, rollabudgetgratis\n"
     ]
    }
   ],
   "source": [
    "# load google news word2vec\n",
    "E = WordEmbedding('./embeddings/sv-word2vec-vectors-nlpl-eu-69/model.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define gender direction\n",
    "\n",
    "We define gender direction by the direciton of she - he because they are frequent and do not have fewer alternative word senses (e.g., man can also refer to mankind). In the paper, we discuss alternative approach for defining gender direction (e.g., using PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender direction\n",
    "v_gender = E.diff('hon', 'han')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generating analogies of \"Man: x :: Woman : y\"\n",
    "\n",
    "We show that the word embedding model generates gender-streotypical analogy pairs. \n",
    "To generate the analogy pairs, we use the analogy score defined in our paper. This score finds word pairs that are well aligned with gender direction as well as within a short distance from each other to preserve topic consistency. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing neighbors\n",
      "Mean: 842.0646\n",
      "Median: 717.0\n"
     ]
    }
   ],
   "source": [
    "# analogies gender\n",
    "a_gender = E.best_analogies_dist_thresh(v_gender)\n",
    "\n",
    "for (a,b,c) in a_gender:\n",
    "    print(a+\"-\"+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Analyzing gender bias in word vectors asscoiated with professions\n",
    "\n",
    "Next, we show that many occupations are unintendedly associated with either male of female by projecting their word vectors onto the gender dimension. \n",
    "\n",
    "The script will output the profession words sorted with respect to the projection score in the direction of gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profession analysis gender\n",
    "sp = sorted([(E.v(w).dot(v_gender), w) for w in profession_words])\n",
    "\n",
    "sp[0:20], sp[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Racial Bias\n",
    "\n",
    "### Step 5: Define racial direction\n",
    "We define racial direction based on the common names in different Demographic groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Emily\", \"Aisha\", \"Anne\", \"Keisha\", \"Jill\", \"Tamika\", \"Allison\", \"Lakisha\", \"Laurie\", \"Tanisha\", \"Sarah\",\n",
    "         \"Latoya\", \"Meredith\", \"Kenya\", \"Carrie\", \"Latonya\", \"Kristen\", \"Ebony\", \"Todd\", \"Rasheed\", \"Neil\", \"Tremayne\",\n",
    "         \"Geoffrey\", \"Kareem\", \"Brett\", \"Darnell\", \"Brendan\", \"Tyrone\", \"Greg\", \"Hakim\", \"Matthew\", \"Jamal\", \"Jay\",\n",
    "         \"Leroy\", \"Brad\", \"Jermaine\"]\n",
    "names_group1 = [names[2 * i] for i in range(len(names) // 2)]\n",
    "names_group2 = [names[2 * i + 1] for i in range(len(names) // 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# racial direction\n",
    "vs = [sum(E.v(w) for w in names) for names in (names_group2, names_group1)]\n",
    "vs = [v / np.linalg.norm(v) for v in vs]\n",
    "\n",
    "v_racial = vs[1] - vs[0]\n",
    "v_racial = v_racial / np.linalg.norm(v_racial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Generating racial biased analogies\n",
    "\n",
    "Similar to Step 3, we generate analogies that align with the racial dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# racial analogies\n",
    "a_racial = E.best_analogies_dist_thresh(v_racial)\n",
    "\n",
    "for (a,b,c) in a_racial:\n",
    "    print(a+\"-\"+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Analyzing racial bias in word vectors asscoiated with professions\n",
    "\n",
    "Similar to Step 4, we project occpurations onto the racial dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profession analysis racial\n",
    "sp = sorted([(E.v(w).dot(v_racial), w) for w in profession_words])\n",
    "\n",
    "sp[0:20], sp[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Repeat Step 2-4 with debiased word embedding. \n",
    "\n",
    "You can use debiaswe debias function to do the debiasing with word sets of your choosing\n",
    "\n",
    "You can leave equalize_pairs and gender_specific_words blank when coming up with your own groups. We give an example for the case of gender below for you to warm up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debiaswe.debias import debias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load some gender related word lists to help us with debiasing\n",
    "with open('./data/definitional_pairs.json', \"r\") as f:\n",
    "    defs = json.load(f)\n",
    "print(\"definitional\", defs)\n",
    "\n",
    "with open('./data/equalize_pairs.json', \"r\") as f:\n",
    "    equalize_pairs = json.load(f)\n",
    "\n",
    "with open('./data/gender_specific_seed.json', \"r\") as f:\n",
    "    gender_specific_words = json.load(f)\n",
    "print(\"gender specific\", len(gender_specific_words), gender_specific_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debias(E, gender_specific_words, defs, equalize_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profession analysis gender\n",
    "sp_debiased = sorted([(E.v(w).dot(v_gender), w) for w in profession_words])\n",
    "\n",
    "sp_debiased[0:20], sp_debiased[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analogies gender\n",
    "a_gender_debiased = E.best_analogies_dist_thresh(v_gender)\n",
    "\n",
    "for (a,b,c) in a_gender_debiased:\n",
    "    print(a+\"-\"+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
